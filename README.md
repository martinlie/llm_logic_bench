# llm_logic_bench

llm_logic_bench is a lightweight test harness that runs a small set of predefined logical statements (sourced from logic_bench) against different LLM models. It provides a simple way to compare model responses on controlled logic tasks and evaluate consistency and reasoning quality.

Prepare by executing: 

      git clone https://github.com/Mihir3009/LogicBench.git

in terminal to download LogicBench files into the repo. Add 'SCW_OPENAI_SECRET_KEY' (Scaleway Secret Key) to the environment or a .env file.

Run with:

      uvx git+https://github.com/you/llm_logic_bench

or just create an environment and run the 'main.py' file.

References:
* https://github.com/Mihir3009/LogicBench
* Parmar, M., Patel, N., Varshney, N., Nakamura, M., Luo, M., Mashetty, S., Mitra, A., & Baral, C. (2024). LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models. arXiv. https://doi.org/10.48550/ARXIV.2404.15522

